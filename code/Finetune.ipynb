{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "\n",
    "# Load your custom dataset\n",
    "dataset = load_dataset(\"path_to_your_dataset\")\n",
    "\n",
    "# Load processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\", \n",
    "                                       vocab_size=processor.tokenizer.vocab_size)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(batch):\n",
    "    # Load audio\n",
    "    audio = torchaudio.load(batch[\"audio_path\"])[0]\n",
    "    batch[\"input_values\"] = processor(audio.numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"], return_tensors=\"pt\").input_ids[0]\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(preprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-dali\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./wav2vec2-dali\")\n",
    "processor.save_pretrained(\"./wav2vec2-dali\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
