{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/Users/fwtrv/Downloads/Jump Up, Super Star! NDC Festival Edition.wav\"\n",
    "lyrics_path = \"/Users/fwtrv/Desktop/ECE-GY 7123/Final/Jump Up, Super Star! NDC Festival Edition.txt\"\n",
    "save_track_vocal_path = \"track_vocal.wav\"\n",
    "# lang=\"zh-CN\"\n",
    "lang = \"en-US\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import torch\n",
    "import demucs\n",
    "import librosa\n",
    "import soundfile\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Selected: mps\n"
     ]
    }
   ],
   "source": [
    "# Select device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"     # CUDA\n",
    "elif torch.backends.mps.is_available(): \n",
    "    device = torch.device('mps')    # Apple Sillicon\n",
    "else:\n",
    "    device = torch.device(\"cpu\")    # CPU\n",
    "\n",
    "print(\"Device Selected:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio File Preprocessing - Extract Vocals from Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Vocals from Audio File\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "from demucs.separate import load_track\n",
    "ORIGINAL_SR = 44100\n",
    "TARGET_SR = 16000\n",
    "\n",
    "# Choose Demucs Model for Vocals Extraction\n",
    "demucs_model = get_model(name=\"htdemucs\", repo=None)\n",
    "demucs_model.to(device)\n",
    "demucs_model.eval()\n",
    "vocals_source_idx = demucs_model.sources.index(\"vocals\")\n",
    "sample_rate = demucs_model.samplerate\n",
    "\n",
    "# Load Aduio Track\n",
    "audio_track = load_track(audio_path, 2, sample_rate)\n",
    "\n",
    "# Extract Vocal\n",
    "ref = audio_track.mean(0)\n",
    "audio_track_nor = (audio_track - ref.mean()) / ref.std() # Normalization\n",
    "with torch.no_grad():\n",
    "    sources = apply_model(demucs_model, audio_track_nor[None], device=device, shifts=1, split=True, overlap=0.25, progress=False)\n",
    "track_vocal = sources[0][vocals_source_idx].cpu().numpy()[0, ...]\n",
    "\n",
    "# Post-processing\n",
    "track_vocal = librosa.resample(track_vocal, orig_sr=ORIGINAL_SR, target_sr=TARGET_SR)\n",
    "\n",
    "# Write to Output\n",
    "soundfile.write(save_track_vocal_path, track_vocal, TARGET_SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE|WE|GO,|OFF|THE|RAILS|DON'T|YOU|KNOW|IT'S|TIME|TO|RAISE|OUR|SAILS|IT'S|FREEDOM|LIKE|YOU|NEVER|KNEW||DON'T|NEED|BAGS,|OR|A|PASS|SAY|THE|WORD|I'LL|BE|THERE|IN|A|FLASH|YOU|COULD|SAY|MY|HAT|IS|OFF|TO|YOU||OH|WE|CAN|ZOOM|ALL|THE|WAY|TO|THE|MOON|FROM|THIS|GREAT|WIDE|WACKY|WORLD|JUMP|WITH|ME,|GRAB|COINS|WITH|ME|OH|YEAH!||IT'S|TIME|TO|JUMP|UP|IN|THE|AIR|(JUMP|UP|IN|THE|AIR)|JUMP|UP,|DON'T|BE|SCARED|(JUMP|UP,|DON'T|BE|SCARED)|JUMP|UP|AND|YOUR|CARES|WILL|SOAR|AWAY|AND|IF|THE|DARK|CLOUDS|START|TO|SWIRL|(DARK|CLOUDS|START|TO|SWIRL)|DON'T|FEAR,|DON'T|SHED|A|TEAR,|'CAUSE|I'LL|BE|YOUR|1UP|GIRL||SO|LET'S|ALL|JUMP|UP|SUPER|HIGH|(JUMP|UP|SUPER|HIGH)|HIGH|UP|IN|THE|SKY|(HIGH|UP|IN|THE|SKY)|THERE'S|NO|POWER-UP|LIKE|DANCING|YOU|KNOW|THAT|YOU'RE|MY|SUPERSTAR|(YOU'RE|MY|SUPERSTAR)|NO|ONE|ELSE|CAN|TAKE|ME|THIS|FAR|I'M|FLIPPING|THE|SWITCH|GET|READY|FOR|THIS|OH,|LET'S|DO|THE|ODYSSEY||ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|ODYSSEY!||SPIN|THE|WHEEL,|TAKE|A|CHANCE|EVERY|JOURNEY|STARTS|A|NEW|ROMANCE|A|NEW|WORLD'S|CALLING|OUT|TO|YOU||TAKE|A|TURN,|OFF|THE|PATH|FIND|A|NEW|ADDITION|TO|THE|CAST|YOU|KNOW|THAT|ANY|CAPTAIN|NEEDS|A|CREW||TAKE|IT|IN|STRIDE|AS|YOU|MOVE|SIDE|TO|SIDE|THEY'RE|JUST|DIFFERENT|POINTS|OF|VIEW|JUMP|WITH|ME,|GRAB|COINS|WITH|ME|OH|YEAH!||COME|ON|AND|JUMP|UP|IN|THE|AIR|(JUMP|UP|IN|THE|AIR)|JUMP|WITHOUT|A|CARE|(JUMP|WITHOUT|A|CARE)|JUMP|UP|'CAUSE|YOU|KNOW|THAT|I'LL|BE|THERE|AND|IF|YOU|FIND|YOU'RE|SHORT|ON|JOY|(FIND|YOU'RE|SHORT|ON|JOY)|DON'T|FRET,|JUST|DON'T|FORGET|THAT|YOU'RE|STILL|OUR|1UP|BOY||SO|GO|ON|STRAIGHTEN|UP|YOUR|CAP|(STRAIGHTEN|UP|YOUR|CAP)|LET|YOUR|TOES|BEGIN|TO|TAP|(TOES|BEGIN|TO|TAP)|THIS|RHYTHM|IS|A|POWER|'SHROOM|DON'T|FORGET|YOU'RE|THE|SUPERSTAR|(YOU'RE|THE|SUPERSTAR)|NO|ONE|ELSE|CAN|MAKE|IT|THIS|FAR|PUT|A|COMB|THROUGH|THAT|'STACHE|NOW|YOU'VE|GOT|PANACHE|OH,|LET'S|DO|THE|ODYSSEY||ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|YA|SEE!|ODYSSEY,|ODYSSEY!\n"
     ]
    }
   ],
   "source": [
    "# Get plain lyrics from file\n",
    "with open(lyrics_path, 'r') as file:\n",
    "    lyrics_plain = file.read()\n",
    "\n",
    "lyrics_processed = \"\"\n",
    "if lang == \"en-US\":\n",
    "    lyrics_processed = lyrics_plain.upper()\n",
    "    lyrics_processed = lyrics_processed.replace(' ', '|')\n",
    "    lyrics_processed = lyrics_processed.replace('\\n', '|')\n",
    "    lyrics_processed = lyrics_processed.replace('_', '\\'')\n",
    "    lyrics_processed = lyrics_processed.replace('â€™', '\\'')\n",
    "\n",
    "print(lyrics_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, logging, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Select Model\n",
    "model_id = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "if lang == \"zh-CN\":\n",
    "    model_id = 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn'\n",
    "\n",
    "# Load Model\n",
    "Wav2Vec2_model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "Wav2Vec2_processor = Wav2Vec2Processor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneme_recognizer(vocals, processor, model):\n",
    "    vals = processor(vocals, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
    "    duration_sec = vals.shape[1] / TARGET_SR\n",
    "    with torch.no_grad():\n",
    "        logits = model(vals).logits\n",
    "    return logits.cpu().detach(), duration_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segementation from (4152960,) to (17, 240000)\n",
      "[\"BARAPARATA PAPARAT HERE WE GO OF THE RAILS YOU KNOW IT'S TIME TO RAISE OURSELVS IT'S FREEDOM LIKE YOU NEVER KNEW E AD O THE PAST SAY THE WORD I'LL BE ANA FLASH YOU COULD SAY MY HATES OYOU O WE CONSUME ON THE WAY TO THE MOONTHIS GREAT WIDE WAY WORLD JUMP WITH ME GROW HORNS WITH ME  IT'S TIME TO JUMP UP IN THE A JUMPUP DON'T BE STRAT TUMP UP AND YOUR CHAIRS WILL SOR AWAY AND IN THE DAROFTE O O IT I'LL BE O ONE AGIR SO LET OM JUMP UP SUPERA HIGH UP IN THE SKY THE SNOW OEOP MY ANY YOU KNOW THAT YOU'RE MY SUPERSTI NO WHENIN POPING THE PIGE GET READY FOR THELES O TE ODES I MUST SAY YE S I SAY HAS I SAY EA SA I SAY HEA SA I SAY HESAY  SAY HS SSS S SPIN THE WHEEL TAKE A CHANCE EVERY TURN HE STARTS A NEW ROMANCE A NEW WORLD'S CALLING OUT TO YOU TAKE A TURN OF THE PATH FIND A NEW ADDITION TO THE CAST YOU KNOW THAT ANY CAPTAIN NEEDS A CREW TAKEIN STAMO DIFFERENT POINTS OF VIEW JUMP WITH ME GRAND COINS WITH ME A COME ON JUMP U IN THE AREP WITHOUT A CHA  CAT O O TA IL BE THE AND INFINE SO JOLIS TYOU'RE STILL A ON A BOY SO GOON STA YO TRY BUT YOUR TONGS BEGIN TO TAL THI RITM IS AFARE DON'T FORGET YOUR SUPER STAS NO WATI PO FOR TE COMFO THAT NOW YOUVE GOT ENALE THE ONEST  SAY YES  AI SAY YES I SAY I SAY ES I SA ES I SA ES  SAY S ISA SA O HERE WE GO OF THE RAILS YOU KNOW IT'S TIME TO RAISE OURSELVS IT'S FREEDOM LIKE YOU NEVER KNEW SNEAK LAD OF TE PAST SAY THE WORD I'LL BE THERE IN A FLASH YOU CAN SAY MY HAPPY\"]\n"
     ]
    }
   ],
   "source": [
    "duration_sec = 0\n",
    "\n",
    "# Segmentation\n",
    "# Each column track_segments[:, i] contains a contiguous slice of the input track_vocal[i * hop_length : i * hop_length + frame_length]\n",
    "track_segments = librosa.util.frame(track_vocal, frame_length=int(TARGET_SR * 15), hop_length=int(TARGET_SR * 15), axis=0)\n",
    "print(\"Segementation from \" + str(track_vocal.shape) + \" to \" + str(track_segments.shape))\n",
    "\n",
    "# Get Raw Prediction\n",
    "# logits: the vector of raw (non-normalized) predictions\n",
    "duration_sec = 0\n",
    "logits, duration_temp = phoneme_recognizer(track_segments[0], Wav2Vec2_processor, Wav2Vec2_model)\n",
    "duration_sec += duration_temp\n",
    "for seg in track_segments[1:]:\n",
    "    logits_seg, duration_temp = phoneme_recognizer(seg, Wav2Vec2_processor, Wav2Vec2_model)\n",
    "    duration_sec += duration_temp\n",
    "    # Concatenates the logits\n",
    "    logits = torch.cat((logits, logits_seg), dim=1)\n",
    "\n",
    "# Normalize Results\n",
    "emission = torch.log_softmax(logits, dim=-1)[0].cpu().detach()\n",
    "\n",
    "# Get Prediction Results\n",
    "pred = torch.argmax(logits, dim=-1)\n",
    "transcription = Wav2Vec2_processor.batch_decode(pred)\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "SEPARATOR = '|'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    prob: float\n",
    "\n",
    "def seconds_to_lrc(seconds, is_word = True):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = seconds % 60\n",
    "    hundredths = int((seconds % 1) * 100)\n",
    "    seconds = int(seconds)\n",
    "    formated = f\"{minutes:02d}:{seconds:02d}.{hundredths:02d}\"\n",
    "    return f\"<{formated}>\" if is_word else f\"[{formated}]\"\n",
    "\n",
    "@dataclass\n",
    "class Word:\n",
    "    label: str\n",
    "    start: float\n",
    "    end: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{seconds_to_lrc(self.start)} {self.label}\"\n",
    "\n",
    "class LrcFormatter():\n",
    "    @staticmethod\n",
    "    def words2lrc(words: List[Word], original_lyrics: str, lang=\"en-US\"):\n",
    "        lrc = \"\"\n",
    "        counter = 0\n",
    "        word_end = None\n",
    "        for line in original_lyrics.splitlines():\n",
    "            if line == '': continue\n",
    "            if word_end:\n",
    "                lrc += f\"\\n{seconds_to_lrc(word_end, False)}\"\n",
    "            else:\n",
    "                lrc += \"[00:00.00]\"\n",
    "            if lang == \"en-US\":\n",
    "                splitted_words = line.split(' ')\n",
    "            elif lang == \"zh-CN\":\n",
    "                splitted_words = line\n",
    "\n",
    "            for original_word in splitted_words:\n",
    "                if original_word == '': continue\n",
    "                word = words[counter]\n",
    "                word.label = original_word\n",
    "                lrc += f\" {word}\"\n",
    "                word_end = word.end\n",
    "                counter+=1\n",
    "        return lrc\n",
    "\n",
    "class Aligner():\n",
    "    @staticmethod\n",
    "    def align(emission, tokens, blank_id=0):\n",
    "        trellis = Aligner.get_trellis(emission, tokens, blank_id=blank_id)\n",
    "        path = Aligner.backtrack(trellis, emission, tokens, blank_id=blank_id)\n",
    "        return path\n",
    "\n",
    "    @staticmethod\n",
    "    def get_trellis(emission, tokens, blank_id=0):\n",
    "        num_frame = emission.size(0)\n",
    "        num_tokens = len(tokens)\n",
    "        trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
    "        trellis[0, 0] = 0\n",
    "        trellis[1:, 0] = torch.cumsum(emission[:, blank_id], 0)\n",
    "        trellis[0, -num_tokens:] = -float(\"inf\")\n",
    "        trellis[-num_tokens:, 0] = float(\"inf\")\n",
    "\n",
    "        for t in range(num_frame):\n",
    "            trellis[t + 1, 1:] = torch.maximum(\n",
    "                # Score for staying at the same token\n",
    "                trellis[t, 1:] + emission[t, blank_id],\n",
    "                # Score for changing to the next token\n",
    "                trellis[t, :-1] + emission[t, tokens],\n",
    "            )\n",
    "        return trellis\n",
    "\n",
    "    @staticmethod\n",
    "    def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "        j = trellis.size(1) - 1\n",
    "        t_start = torch.argmax(trellis[:, j]).item()\n",
    "\n",
    "        path = []\n",
    "        for t in range(t_start, 0, -1):\n",
    "            stayed = trellis[t - 1, j] + emission[t - 1, blank_id]\n",
    "            changed = trellis[t - 1, j - 1] + emission[t - 1, tokens[j - 1]]\n",
    "            prob = emission[t - 1, tokens[j - 1]\n",
    "                            if changed > stayed else 0].exp().item()\n",
    "            path.append(Point(j - 1, t - 1, prob))\n",
    "            if changed > stayed:\n",
    "                j -= 1\n",
    "                if j == 0:\n",
    "                    break\n",
    "        else:\n",
    "            raise Exception(\"Failed\")\n",
    "        return path[::-1]\n",
    "\n",
    "    def get_words_from_path(text, path, frame_duration):\n",
    "        # Skip repeating char\n",
    "        i1, i2 = 0, 0\n",
    "        segments = []\n",
    "        while i1 < len(path):\n",
    "            while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "                i2 += 1\n",
    "            segments.append(\n",
    "                Segment(\n",
    "                    text[path[i1].token_index],\n",
    "                    path[i1].time_index,\n",
    "                    path[i2 - 1].time_index + 1\n",
    "                )\n",
    "            )\n",
    "            i1 = i2\n",
    "        if lang == \"en-US\":\n",
    "            return Aligner.merge_en(segments, frame_duration)\n",
    "        elif lang == \"zh-CN\":\n",
    "            return [Word(s.label, s.start * frame_duration, s.end * frame_duration) for s in segments]\n",
    "        \n",
    "\n",
    "    def merge_en(segments, frame_duration, separator=SEPARATOR):\n",
    "        # Merge chars to word\n",
    "        words = []\n",
    "        i1, i2 = 0, 0\n",
    "        while i1 < len(segments):\n",
    "            if i2 >= len(segments) or segments[i2].label == separator:\n",
    "                if i1 != i2:\n",
    "                    segs = segments[i1:i2]\n",
    "                    word = \"\".join([seg.label for seg in segs])\n",
    "                    words.append(Word(\n",
    "                        word, segments[i1].start * frame_duration, segments[i2 - 1].end * frame_duration))\n",
    "                i1 = i2 + 1\n",
    "                i2 = i1\n",
    "            else:\n",
    "                i2 += 1\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares text labels for the CTC\n",
    "lyrics_tokens = Wav2Vec2_processor.tokenizer(lyrics_processed).input_ids\n",
    "\n",
    "path = Aligner.align(emission, lyrics_tokens)\n",
    "\n",
    "words = Aligner.get_words_from_path(\n",
    "            text=lyrics_processed, path=path, frame_duration=Wav2Vec2_model.config.inputs_to_logits_ratio / TARGET_SR)\n",
    "\n",
    "# lrc\n",
    "lrc = LrcFormatter.words2lrc(words, lyrics_plain)\n",
    "# with open(f'results.lrc', 'w+') as fp:\n",
    "#     fp.write(lrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0].label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
